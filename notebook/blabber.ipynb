{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blabber Cleaning\n",
    "#### (By: Mark Ehab Aziz)\n",
    "#### (Built Under: Python 3.11.4)\n",
    "Filtering out and cleaning text data.\n",
    "As tasked inside the 'to do.txt'.\n",
    "\n",
    "Ensure the presence of nltk package using `pip install nltk`.\n",
    "\n",
    "Following usage of nltk should not require further dependencies than the basic install and stopwords.\n",
    "\n",
    "If anything; Ensure presence of `nltk`, the download for stopwords is within the cells and will download automatically should it not detect any instance of predownloaded stopwords for itself.\n",
    "\n",
    "## Note:\n",
    "You may encounter (Window Not Responding), in which case; kindly wait for it, as the notebook's size seems to increase by a lot after running the stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd                         # Loading Data\n",
    "import nltk                                 # Required to download stopwords set\n",
    "from nltk.corpus import stopwords           # Load Stopwords\n",
    "from nltk.tokenize import regexp_tokenize   # To Tokenize words with Regex Expressions\n",
    "from nltk.stem import PorterStemmer         # Stemming words\n",
    "from nltk.stem import SnowballStemmer       # Improved Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into environment\n",
    "# Using two methods (As stated in my previous projects)\n",
    "# 1. Path working within my git repo\n",
    "blab = pd.read_csv(\"../dataset/train.csv\")\n",
    "\n",
    "# 2. Path when data is within the same folder\n",
    "#blab = pd.read_csv(\"./train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "Using `.head(n)` to show the first $n^{th}$ rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining n rows to see\n",
    "n = 5\n",
    "\n",
    "# Showing head\n",
    "blab.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated by our todo list, we are only tasked with cleaning of the text, so we'll be focusing on `comment_text`.\n",
    "\n",
    "Referring to our todo list once again, we will be dropping `id`, `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, and `identity_hate`; as we are not concerned with classifying the sentiment or the meaning behind any of the comments.\n",
    "\n",
    "Reminder for what to be done:\n",
    "- Read Text\n",
    "- Clean Text (Capitalisation, punctuation)\n",
    "- Remove Stop Words\n",
    "- Tokenization\n",
    "- Stemming\n",
    "\n",
    "Under no aforementioned task will we be using the columns I have mentioned to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text\n",
       "0  Explanation\\r\\nWhy the edits made under my use...\n",
       "1  D'aww! He matches this background colour I'm s...\n",
       "2  Hey man, I'm really not trying to edit war. It...\n",
       "3  \"\\r\\nMore\\r\\nI can't make any real suggestions...\n",
       "4  You, sir, are my hero. Any chance you remember..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining list of columns to be dropped\n",
    "col_droppable = [\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "# Dropping\n",
    "txt_blab = blab.drop(columns = col_droppable)\n",
    "\n",
    "# Viewing\n",
    "txt_blab.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation  Why the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"  More  I can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text\n",
       "0  Explanation  Why the edits made under my usern...\n",
       "1  D'aww! He matches this background colour I'm s...\n",
       "2  Hey man, I'm really not trying to edit war. It...\n",
       "3  \"  More  I can't make any real suggestions on ...\n",
       "4  You, sir, are my hero. Any chance you remember..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing '\\n' '\\r' '\\t' from every line\n",
    "txt_blab.replace(r'[\\r\\n\\t]', ' ', regex = True, inplace=True)\n",
    "\n",
    "# As noted, there are no escape characters for spaces, as new line or tab\n",
    "txt_blab.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Above Sentences\n",
    "Using the NLTK library for Python; will be copy-pasting or creating patterns that are enough to extract words, starting with either upper or lower case letters.\n",
    "\n",
    "This may violate the order of operations specified in the ToDo list, as cleaning data preceeds tokenization, but `regexp_tokenize()` takes care of both steps anyway, through just matching what is specified within the regex, as only 'Latin Alphabet' ranges are specified (`A-Za-z`), it will automatically unmatch any special character or non-alphabet character, ignores punctuation as well.\n",
    "\n",
    "Will also be removing the URLs as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Regex patterns\n",
    "# Match words starting with Uppercase letters\n",
    "upper_words = r\"([A-Z])\\w+\"\n",
    "\n",
    "# Match Words that start with either Upper/lowercase letters\n",
    "upper_lower_words = r\"[A-Za-z]\\w+\"\n",
    "\n",
    "# Match URLs\n",
    "url_pattern = r\"(http|ftp|https):\\/\\/([\\w+?\\.\\w+])+([a-zA-Z0-9\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)_\\-\\=\\+\\\\\\/\\?\\.\\:\\;\\'\\,]*)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation  Why the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"  More  I can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text\n",
       "0  Explanation  Why the edits made under my usern...\n",
       "1  D'aww! He matches this background colour I'm s...\n",
       "2  Hey man, I'm really not trying to edit war. It...\n",
       "3  \"  More  I can't make any real suggestions on ...\n",
       "4  You, sir, are my hero. Any chance you remember..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing URLs (Standard URL Scheme, There still exist instances\n",
    "# of just 'https' or 'http' randomly written, they will just be\n",
    "# treated like normal words and tokenized as the rest)\n",
    "txt_blab.replace(url_pattern, '', regex = True, inplace = True)\n",
    "\n",
    "txt_blab.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Iterating over each row of the given textual data, accessing as a string instead of a usual row in order to yield the full entry.\n",
    "\n",
    "Using regex to tokenize words by matching pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a list of tokens, to hold tokens of each entry\n",
    "# Probably better to use a dictionary if we care about count (?)\n",
    "# Still have to access every token and change to lower (Taken care of in flat list)\n",
    "token_per_row = []\n",
    "\n",
    "# Start and finish indecies of iterator\n",
    "# Bound to become the length of the file eventually\n",
    "for i in range(txt_blab.shape[0]):\n",
    "    # Grab string fully from dataframe\n",
    "    line = txt_blab.iloc[i,0]\n",
    "\n",
    "    # Append list of tokens\n",
    "    # 2D list of lists; each containing tokens of each row\n",
    "    token_per_row.append(regexp_tokenize(line, upper_lower_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 2D List of Lists\n",
    "# n^2 operation but still gets the job done\n",
    "# Would be better to flatten as soon as\n",
    "# the tokens are fresh out the tokenizer\n",
    "def flatten(list_o_lists):\n",
    "    # init flat list\n",
    "    flat = []\n",
    "\n",
    "    # Loop over every list within the list\n",
    "    for sublist in list_o_lists:\n",
    "        # Loop over every token within the sublist\n",
    "        # being iterated on\n",
    "        for token in sublist:\n",
    "            # Append token to flat list\n",
    "            flat.append(token)\n",
    "\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Explanation', 'Why', 'the', 'edits', 'made', 'under', 'my', 'username', 'Hardcore', 'Metallica', 'Fan', 'were', 'reverted', 'They', 'weren', 'vandalisms', 'just', 'closure', 'on', 'some']\n"
     ]
    }
   ],
   "source": [
    "# Call the List flatter\n",
    "flat_tokens = flatten(token_per_row)\n",
    "\n",
    "print(flat_tokens[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the stopwords\n",
    "# Already installed so will comment it out\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Need to remove stop words too\n",
    "# changing the stopwords from a list to a set (Performance Upgrade)\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopword Removal\n",
    "Prior to removing stopwords, one has to change the case of the tokens (words) to be lowercase, which is also what is asked of us to do within the ToDo list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation\n"
     ]
    }
   ],
   "source": [
    "# Changing all words into lowercase\n",
    "# Using a list comprehension to change it more efficienty\n",
    "# (They're better than for loops)\n",
    "flat_tokens = [token.lower() for token in flat_tokens]\n",
    "\n",
    "# Was 'Explanation', should be 'explanation'\n",
    "print(flat_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5425847\n",
      "10130248\n"
     ]
    }
   ],
   "source": [
    "# Actually removing stopwords\n",
    "stop_free = [token for token in flat_tokens if token not in ENGLISH_STOPWORDS]\n",
    "\n",
    "# Getting how many words remained after removal of stopwords\n",
    "print(len(stop_free))\n",
    "\n",
    "# Getting how many words were tokenized (Both stop and non-stop)\n",
    "print(len(flat_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Changing the word back to its roots. Through using the 'Porter Algorithm'. (Fast and Effective, Not very accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of list 1: 1000000\n",
      "Length of list 2: 1000000\n",
      "Length of list 3: 1000000\n",
      "Length of list 4: 500000\n",
      "Length of list 5: 1000000\n",
      "Length of list 6: 425847\n"
     ]
    }
   ],
   "source": [
    "# Using porterstemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Inserting the semmed words into a list\n",
    "# Will be dividing into \"batches\" due to\n",
    "# reaching maximum recursion depth if\n",
    "# if all entries are sent at once\n",
    "stemmed_words0 = [stemmer.stem(word) for word in stop_free[:1000000]]\n",
    "print('Length of list 1: {}'.format(len(stemmed_words0)))\n",
    "stemmed_words1 = [stemmer.stem(word) for word in stop_free[1000000:2000000]]\n",
    "print('Length of list 2: {}'.format(len(stemmed_words1)))\n",
    "stemmed_words2 = [stemmer.stem(word) for word in stop_free[2000000:3000000]]\n",
    "print('Length of list 3: {}'.format(len(stemmed_words2)))\n",
    "\n",
    "# Explaining the 500k word skip:\n",
    "# Kernel would throw an error due to \"Reaching Maximum Recursion Depth\"\n",
    "# on coming across a certain word which seems to have it lock up\n",
    "# after changing the indecies a little, this configuration works best\n",
    "# could probably be fine tuned to find the word that messes it up\n",
    "stemmed_words3 = [stemmer.stem(word) for word in stop_free[3000000:3500000]]\n",
    "print('Length of list 4: {}'.format(len(stemmed_words3)))\n",
    "\n",
    "stemmed_words4 = [stemmer.stem(word) for word in stop_free[4000000:5000000]]\n",
    "print('Length of list 5: {}'.format(len(stemmed_words4)))\n",
    "stemmed_words5 = [stemmer.stem(word) for word in stop_free[5000000:]]\n",
    "print('Length of list 6: {}'.format(len(stemmed_words5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stemmed words: 4925847\n",
      "['explan', 'edit', 'made', 'usernam', 'hardcor', 'metallica', 'fan', 'revert', 'vandal', 'closur']\n"
     ]
    }
   ],
   "source": [
    "# Joining lists\n",
    "total_stemmed_words = stemmed_words0 + stemmed_words1 + stemmed_words2 + stemmed_words3 + stemmed_words4 + stemmed_words5\n",
    "\n",
    "# Printing number of words\n",
    "print('Total number of stemmed words: {}'.format(len(total_stemmed_words)))\n",
    "\n",
    "# Displaying some words\n",
    "print(total_stemmed_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a lot of words are either missing an e at the end, or not even english anymore, that is due to Stemmer using a crude old method, which is aimed for speed and efficiency, unlike lemmatizaton which morphologically analyses lexical changes in words to revert them back to their roots, unlike the chopping of \"commonly found prefixes/suffixes\" which stemming does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit more searching and digging around, within the NLTK package there exists another variant of the PorterStemmer called SnowballStemmer, which fixes the above issues regarding missing an e or plain out non-english words.\n",
    "\n",
    "It will be used over the porterstemmer for the following tasks, whilst keeping the porter stemmer cells to highlight the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Continuation\n",
    "Relevant tasks were assigned to be done on the same notebook, the are present within the `README.md`, but as a quick reminder I will list them here.\n",
    "\n",
    "1. Bag of Words\n",
    "2. Word Embeddings\n",
    "3. Use BERT and Evaluate\n",
    "\n",
    "# Variable Definition\n",
    "We will be using the following variables carried over from the previous part of this notebook, namely:\n",
    "- `ENGLISH_STOPWORDS`: Constant for holding the stopwords found in the English language.\n",
    "- `flat_tokens`: Tokens from every row parsed into a single list of tokens.\n",
    "- `stop_free`: List of tokens free of stopwords.\n",
    "- `total_stemmed_words`: List of stemmed words from tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a stemmer object\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "# Iterate over the flattened list of words\n",
    "total_stemmed_words = [snowball.stem(word) for word in stop_free]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5425847\n",
      "['explan', 'edit', 'made', 'usernam', 'hardcor', 'metallica', 'fan', 'revert', 'vandal', 'closur']\n"
     ]
    }
   ],
   "source": [
    "# Printing number of words\n",
    "print(len(total_stemmed_words))\n",
    "\n",
    "# Print some words\n",
    "print(total_stemmed_words[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
